# Interpretability / Attribution

- keywords: Visualisation, Explainability, DeepDream

## Definition
The goal of explainability research, is to understand what a neural network learned and why it makes the decisions it makes.
Exploring variation in what neurons react to, how they interact, and how to improve the optimization process. 

Attribution studies what part of an example is responsible for the network activating a particular way. ("attribution” and “saliency maps")

- Gradient optimized images dominated by high frequencies - https://arxiv.org/pdf/1506.06579


## Methods

Sources: https://www.frontiersin.org/articles/10.3389/frai.2022.871162/full,  Springenberg et al., 2014

- Saliency maps
- gradcam (layer activation with guided backprop) 
- Guided backpropagation - The GBP is a gradient-based visualization technique that visualizes the gradient with respect to images when backpropagating through the Relu activation function









- Transparency of deep neural networks for medical image analysis: A review of interpretability methods \cite{salahuddin2022transparency}
- Explainable artificial intelligence: a comprehensive review \cite{minh2022explainable}
- Visualizing Higher-Layer Features of a Deep Network % https://www.researchgate.net/publication/265022827_Visualizing_Higher-Layer_Features_of_a_Deep_Network