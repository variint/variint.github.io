# Performance metrics

## Introduction


### Description


### Goals
* Learn about performance metrics


## Classification

### Accuracy
- Proportion of true results among all cases examined.

### Precision
- Proportion of true positives among all positive predictions.

### Recall (Sensitivity)
- Proportion of true positives among all actual positives.

### F1 Score
- Harmonic mean of precision and recall.

### Area Under the Curve (AUC)
- Area under the ROC curve, indicating classifier performance.

### Log Loss
- Measures the performance of a classification model based on probability values.

### Confusion Matrix
- Table showing true positives, true negatives, false positives, and false negatives.

### Kappa Statistic
- Measures agreement between predicted and observed categorizations, adjusting for chance.

### Precision-Recall AUC
- Area under the precision-recall curve, useful for imbalanced datasets.

### Lift and Gain Charts
- Visual tools for comparing model performance to random guessing.

### Gini Coefficient
- Measures the inequality of a frequency distribution, often used in credit scoring.



## Segmentation
### Intersection over Union (IoU)
- Measures the overlap between predicted and ground truth segments.

### Dice Coefficient
- Measures the similarity between predicted and ground truth segments.

### Pixel Accuracy
- Proportion of correctly classified pixels out of total pixels.

### Mean Pixel Accuracy
- Average pixel accuracy across all classes.



## Regression
### Mean Absolute Error (MAE)
- Average of the absolute differences between predicted and actual values.

### Mean Squared Error (MSE)
- Average of the squared differences between predicted and actual values.

### Root Mean Squared Error (RMSE)
- Square root of the mean squared error, in the same units as the target variable.

### R-squared (Coefficient of Determination)
- Proportion of variance in the dependent variable predictable from the independent variables.

### Median Absolute Error
- Median of the absolute errors between predicted and actual values.

### Mean Absolute Percentage Error (MAPE)
- Average of the absolute percentage errors between predicted and actual values.

### Explained Variance Score
- Measures the proportion of variance in the dependent variable explained by the model.



## Statistics
### Stability Metric
- Measures the consistency of performance across different data subsets or over time.

### Throughput
- Rate at which a system processes data or transactions, used in performance evaluation.


